<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="./jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("|");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="documents/CV_JyotikrishnaDass.pdf">CV (Sep'24)</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/ResearchVision.png" alt="Research Vision" width="300" height="300" class="center" />&nbsp;</td>
<p>
Our research focuses on advancing distributed edge intelligence and optimizing efficiency within the increasingly interconnected digital landscape. We achieve this through the implementation of distributed machine learning, designing scalable algorithms and efficient architectures to democratize artificial intelligence (AI) and foster green AI at the edge. This approach enables local processing of decentralized data streams at source devices while facilitating distributed computations with minimal communication overhead, thereby enhancing the performance, efficiency, and resiliency of machine learning systems.
</p>

<img src="photos/distributed-AI.png" alt="DistributedAI" class="center" />&nbsp;</td>
<p>
Our research strategy is structured around three key areas:
</p>
<ul>
<li><p><b>Distributed Networks:</b> We develop optimization techniques tailored to the unique requirements of edge environments. Our efforts include minimizing synchronization time to optimize processor utilization and reduce latency. Additionally, we have worked on transforming dense, large-scale problems into memory-efficient (sparse) and separable problems suitable for parallel computations.
</p>
</li>
<li><p><b>Parallel Machine Learning Algorithms:</b> Our work focuses on training and updating machine learning models with an emphasis on low memory cost, improved latency, and privacy. We have developed a scalable distributed algorithm that minimizes communication costs for parallelizing model training. Furthermore, we have demonstrated the effectiveness of constructing memory-efficient data summaries and their parallel implementation as potential solutions for accelerating model learning.
</p>
</li>
<li><p><b>Computer Systems:</b> We co-design energy-efficient computing systems to make AI accessible at the edge, promoting green AI. Our contributions include co-designing software and hardware accelerators for energy-efficient distributed training of Support Vector Machines across multiple FPGAs. Additionally, our ViTALiTy project enhances the efficiency of Vision Transformers by overcoming the quadratic cost through the decomposition of attention into low-rank linear and sparse components, followed by the design of a pipelined hardware-level accelerator for fast execution of the static linear Taylor attention component. Our NetDistiller framework improves the task accuracy of Tiny Neural Networks for edge deployment without additional inference overhead. Looking beyond traditional silicon-based computing, our collaboration on ConvLight has resulted in a novel hardware accelerator for convolutional neural networks, leveraging memristor-integrated photonic computing to achieve high performance, low power consumption, and high density.
</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
